{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa3bcb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bda406bab0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as T\n",
    "import shap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from src.regressors.linear_regression import ConsumptionLinearRegressor, ConsumptionMLPRegressor\n",
    "from src.regressors.lstm_regressor import ConsumptionLSTMRegressor\n",
    "from src.regressors.gru_regression import ConsumptionGRURegressor\n",
    "from src.regressors.rnn_regressor import ConsumptionRNNRegressor\n",
    "from src.regressors.conv_rnn_regressor import ConsumptionConvRNNRegressor\n",
    "from src.regressors.transformer_regressor import ConsumptionTransformerRegressor, ConsumptionTSTRegressor\n",
    "from src.regressors.fcn_regressor import ConsumptionFCNRegressor\n",
    "from src.regressors.tcn_regressor import ConsumptionTCNRegressor\n",
    "from src.regressors.resnet_regressor import ConsumptionResNetRegressor\n",
    "\n",
    "from src.pecan_dataport.participant_preprocessing import PecanParticipantPreProcessing\n",
    "from src.dataset import PecanDataset, PecanDataModule\n",
    "from src.utils.functions import create_sequences, mkdir_if_not_exists\n",
    "T.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721aa236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TST\"\n",
    "participant_id = \"661_test_30\"\n",
    "\n",
    "mkdir_if_not_exists('etc/')\n",
    "mkdir_if_not_exists('etc/imgs')\n",
    "mkdir_if_not_exists('etc/imgs/features')\n",
    "mkdir_if_not_exists(f'etc/imgs/features/{participant_id}')\n",
    "mkdir_if_not_exists(f'etc/imgs/features/{participant_id}/shap_values')\n",
    "mkdir_if_not_exists(f'etc/imgs/features/{participant_id}/shap_values/summary_plots')\n",
    "mkdir_if_not_exists(f'etc/imgs/features/{participant_id}/shap_values/summary_plots/{model_name}')\n",
    "mkdir_if_not_exists(f'etc/imgs/features/{participant_id}/shap_values/force_plots')\n",
    "mkdir_if_not_exists(f'etc/imgs/features/{participant_id}/shap_values/force_plots/{model_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467d0588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] - Trainable dataframe shape - (129086, 28)\n",
      "[*] Train dataframe shape: (90360, 28)\n",
      "[*] Validation dataframe shape: (25817, 28)\n",
      "[*] Test dataframe shape: (12909, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 64513/64513 [00:05<00:00, 11033.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 64513/64513 [00:06<00:00, 10673.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 90179/90179 [00:08<00:00, 10764.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 12728/12728 [00:01<00:00, 12172.58it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 25636/25636 [00:02<00:00, 11808.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Train sequence shape: (181, 28)\n",
      "[!] Test sequence shape: (181, 28)\n",
      "[!] Val sequence shape: (181, 28)\n",
      "[!] Len of train, val and test sequence: 90179 25636 12728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pecan_dataset = PecanParticipantPreProcessing('661_test_30', 'data/participants_data/1min/',\n",
    "                                                   181)\n",
    "_, _, train_sequences, test_sequences, val_sequences = pecan_dataset.get_sequences()\n",
    "n_features = pecan_dataset.get_n_features()\n",
    "scaler = pecan_dataset.get_scaler()\n",
    "ckpt = f'checkpoints/participants/661_test_30/sigmoid/{model_name}/best/best-TST-chpkt-pecanstreet-participant-id-661_test_30_epoch=177-val_loss=0.00000.ckpt'\n",
    "device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332444d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 64483/64483 [00:06<00:00, 10236.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 64483/64483 [00:05<00:00, 11927.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((60, 28), (60, 28))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df = pecan_dataset.get_standart_df_features()\n",
    "background_sequence = create_sequences(total_df[:int(len(total_df)*.5)], 'consumption', 60)\n",
    "test_sequence = create_sequences(total_df[int(len(total_df)*.5):], 'consumption', 60)\n",
    "background_sequence[0][0].shape, test_sequence[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfa523c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_sequence = PecanDataset(background_sequence, device)\n",
    "test_sequence = PecanDataset(test_sequence, device)\n",
    "\n",
    "background_data_module = T.utils.data.DataLoader(\n",
    "    background_sequence,\n",
    "    batch_size=512,\n",
    "    shuffle = False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "test_data_module = T.utils.data.DataLoader(\n",
    "    test_sequence,\n",
    "    batch_size=128,\n",
    "    shuffle = False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7ebe643",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = next(iter(background_data_module))\n",
    "background = train_batch[\"sequence\"].to(device)\n",
    "\n",
    "test_batch = next(iter(test_data_module))\n",
    "test = test_batch[\"sequence\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf82815c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 60, 28]), torch.Size([128, 60, 28]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44435e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "n_hidden = 256\n",
    "n_layers = 3\n",
    "dropout = 0.2\n",
    "activation_fn = 'sigmoid'\n",
    "d_model = 128\n",
    "n_head = 16\n",
    "d_ffn = 256\n",
    "tst_activation_fn = 'gelu'\n",
    "sequence_length = 60\n",
    "max_seq_len = 120\n",
    "d_k = 32\n",
    "d_v = 32\n",
    "res_dropout = 0.1\n",
    "fc_dropout = 0.1\n",
    "\n",
    "model = ConsumptionTSTRegressor.load_from_checkpoint(checkpoint_path=ckpt, strict=False, device=device,\n",
    "                                                             n_features=n_features, seq_len=sequence_length,\n",
    "                                        max_seq_len=max_seq_len, d_model=d_model, n_head=n_head,\n",
    "                                        d_k=d_k, d_v=d_v, d_ffn=d_ffn, res_dropout=res_dropout,\n",
    "                                        n_layers=n_layers, lr=lr,  activation_function=tst_activation_fn,\n",
    "                                        fc_dropout=fc_dropout, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50b013ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model.model, background.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a73388b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: unrecognized nn.Module: Flatten\n",
      "Warning: unrecognized nn.Module: GELU\n",
      "Warning: unrecognized nn.Module: GELU\n",
      "Warning: unrecognized nn.Module: GELU\n",
      "Warning: unrecognized nn.Module: Permute\n",
      "Warning: unrecognized nn.Module: Flatten\n",
      "Warning: unrecognized nn.Module: GELU\n",
      "Warning: unrecognized nn.Module: GELU\n",
      "Warning: unrecognized nn.Module: GELU\n",
      "Warning: unrecognized nn.Module: Permute\n",
      "Warning: unrecognized nn.Module: Flatten\n",
      "Warning: unrecognized nn.Module: GELU\n",
      "Warning: unrecognized nn.Module: GELU\n",
      "Warning: unrecognized nn.Module: GELU\n",
      "Warning: unrecognized nn.Module: Permute\n",
      "Warning: unrecognized nn.Module: Flatten\n",
      "Warning: unrecognized nn.Module: GELU\n",
      "Warning: unrecognized nn.Module: GELU\n",
      "Warning: unrecognized nn.Module: GELU\n",
      "Warning: unrecognized nn.Module: Permute\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 6.00 GiB total capacity; 3.61 GiB already allocated; 0 bytes free; 4.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18752/1192603898.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mshap_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mshap_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pvbr0\\anaconda3\\envs\\consumption-forecasting\\lib\\site-packages\\shap\\explainers\\_deep\\__init__.py\u001b[0m in \u001b[0;36mshap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mwere\u001b[0m \u001b[0mchosen\u001b[0m \u001b[1;32mas\u001b[0m \u001b[1;34m\"top\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \"\"\"\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mranked_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_rank_order\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_additivity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_additivity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\pvbr0\\anaconda3\\envs\\consumption-forecasting\\lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py\u001b[0m in \u001b[0;36mshap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;31m# run attribution computation graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[0mfeature_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_output_ranks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[0msample_phis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoint_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[1;31m# assign the attributions to the right part of the output arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pvbr0\\anaconda3\\envs\\consumption-forecasting\\lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, idx, inputs)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[0mselected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pvbr0\\anaconda3\\envs\\consumption-forecasting\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\GitHub\\consumption-forecasting\\src\\models\\tst.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# Encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# z: [bs x q_len x d_model]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# z: [bs x q_len * d_model]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pvbr0\\anaconda3\\envs\\consumption-forecasting\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\GitHub\\consumption-forecasting\\src\\models\\transformers_blocks\\encoder.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pvbr0\\anaconda3\\envs\\consumption-forecasting\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\GitHub\\consumption-forecasting\\src\\models\\transformers_blocks\\encoder.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Multi-Head attention sublayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m## Multi-Head attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0msrc2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;31m## Add & Norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Add: residual connection with residual dropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pvbr0\\anaconda3\\envs\\consumption-forecasting\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\GitHub\\consumption-forecasting\\src\\models\\transformers_blocks\\multi_head_attention.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# Concat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         context = context.transpose(1, 2).contiguous().view(bs, -1,\n\u001b[0m\u001b[0;32m     35\u001b[0m                                                             self.n_heads * self.d_v)  # context: [bs x q_len x n_heads * d_v]\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 6.00 GiB total capacity; 3.61 GiB already allocated; 0 bytes free; 4.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "T.backends.cudnn.enabled = False\n",
    "shap_values = explainer.shap_values(test.to(device))\n",
    "shap_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], pecan_dataset.get_features_names(), show = False, matplotlib=True)\n",
    "plt.savefig(\n",
    "    f'etc/imgs/features/{participant_id}/shap_values/force_plots/{model_name}/{participant_id}_{model_name}_force.svg',\n",
    "    dpi=600, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_2D = shap_values.reshape(-1,28)\n",
    "X_test_2D = test.reshape(-1,28)\n",
    "shap_values_2D.shape, X_test_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf448b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_2d = pd.DataFrame(data=X_test_2D, columns = pecan_dataset.get_features_names())\n",
    "x_test_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa04472",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_2d.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_2D, x_test_2d, show = False)\n",
    "plt.savefig(\n",
    "    f'etc/imgs/features/{participant_id}/shap_values/summary_plots/{model_name}/{participant_id}_{model_name}_bee_summ.svg',\n",
    "    dpi=600, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ff43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_2D, x_test_2d, plot_type=\"bar\", show=False)\n",
    "plt.savefig(\n",
    "    f'etc/imgs/features/{participant_id}/shap_values/summary_plots/{model_name}/{participant_id}_{model_name}_bar_summ.svg',\n",
    "    dpi=600, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c9ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
