## Uma breve história das redes neurais

Adotar máquinas capazes de aprender serve tão somente para executar atividades tão complexas que, até então, somente seres humanos poderiam realizar. Identificar objetos, classificar um determinado estado do ambiente estudado, interpretar uma característica ou, seguindo para um ramo mais avançado, prever o comportamento de uma variável contínua no futuro [[silva-2010]]. Em outras palavras, o aprendizado de máquinas ajuda os seres humanos a automatizar complexas tarefas, a princípio, somente eles poderiam realizar por meio de dados disponíveis para sua execução. Tais dados são fidedignamente representativos à forma como a variável a ser classificada, descrita ou interpretada esta demonstrado no ambiente estudado.  A forma como essa automação se apresenta ao mundo é uma ponderação de pesos e viéses incessantemente ajustados em um procedimento iterativo, ao qual chamamos de treinamento.

Hoje, conceitos de *Deep Learning* e de treinamento por processamento distribuídos são comuns de serem citados em ofertas de emprego e nas empresas *high techs*. Além de verificar o alto poder computacional disponível para tratar problemas com tais soluções, pode-se verificar a conveniência da abordagem do assunto. 

Porém, o conceito, ou mesmo, o domínio de aprendizado de máquinas não surgiu na última década. Foram evoluções conceituais e inúmeras revoluções das formas de obtenção do hoje seria treinamento de redes neurais, sem contar os diversos anos de baixas produções científicas sobre o tema.

Para poder chegar ao presente, o primeiro avanço palpável foi com o modelo de McCulloch-Pittis, de 1943. Seria essa a primeira modelagem matemática de como funcionaria um neurônio biológico, definindo a reação com uma excitação do estado à partir de suas entradas. Percendo uma representação binária do que um neurônio poderia ser,  em 1958, Frank Rosenblatt, revisa o modelo de McCulloch-Pittis estabelecndo que diferentes entradas deveriam ser ponderadas por uma combinação linear que deveria possuir um viés e uma limitador, a função de ativação daquele neurônio. Nasce então o *Perceptron* e modelagem matemática tão familiar no ramos computacional. $$ y=f(\sum_{i=1}^{n}w_{i}x_{i}-\theta) $$, exemplificada pela figura abaixo [[rosenblatt-1957]](***a img é exemplificativa. tem que fazer uma minha***).

![[Pasted image 20220801161233.png]]

A função de ativação, podendo ser do tipo degrau, logística, exponencial ou sigmoidal, definirá o intervalo que a saída poderá adotar. Por exemplo, utilizando uma função de ativação do tipo degrau, o modelo matemático de Rosenblatt poderá ser descrita como na equação $$ 
y=f(\sum_{i=1}^{n}w_{i}x_{i}-\theta)=\begin{cases}
1 & \text{, se } x= \sum_{i=1}^{n}w_{i}x_{i}-\theta \geq 0\\ 
0 & \text{, se } x= \sum_{i=1}^{n}w_{i}x_{i}-\theta < 0
\end{cases} $$
Até a década de 1969 o funcionamento do *Perceptron* seria uma ótima solução, porém, com a publicação de *Perceptrons. An Introduction to Computational Geometry* [[minsky-papert-1969]], Minsky e Papert demonstraram que a representação não era suficiente para a separação de dados não-lineares, o problema XOR, como na Figura abaixo. À partir de então, a pesquisa no campo da inteligência artificial foi aos poucos obtendo poquíssimas publicações, entrando no conhecido "*AI Winter*".  

![[Pasted image 20220801164725.png]]

Mesmo com a pouca aderência dos pesquisadores à tentativa de solucionar o problema XOR para o período, é possível destacar publicações que contribuíram para a continuação do avanço das redes neurais, possibilitando que voltassem a serem estudadas mais amplamente e ocorresse um novo entusiasmo para com o tema. O algoritmo de *backpropagation* de *Werbos* [[werbos-1974]], a implementação da rede *adaptative resonance theory* (ART) de Grossbert, em 1976 [[grossberg-1976]], ao qual trouxe a ideia de mecânismos de atenção e de *reset* para identificação de similaridades entre padrões apresentados e aqueles em que algum neurônio já treinou [[ferreira-silva-1998]], e, por fim, os mapas auto-orgarnizáveis de Kohonen, em 1982 [[kohonen-1976]] formam algumas citações para a contribuição de esse novo entusiasmo.

Tão importante quanto as publicações revolucionárias no ramos de aprendizado de máquinas, as melhores condições de poder computacional da década de 1980 e a construção de algoritmos de otimização, fizeram com que o estudo de redes neurais voltasse ao patamar de incentivo e entusiasmo antes de 1969 [[silva-2010]]. Porém, a introdução das redes recorrentes de Hopfield, em 1982 [[hopfield-1982]], e a utilização do algoritmo *backpropagation* em redes multicamadas, originando o treinamento *feedforward* [[rumelhart-1986]], merecem um maior destaque, seja pela sua maior adoção ou pela abordagem do trabalho aqui descrito.

O trabalho de Hayakin, ao qual apresenta o treinamento de redes multicamadas com ativação de não-linear, é a essência do treinamento de redes de aprendizado profundo [[hayakin-1994]]. À partir de então, avanços foram tidos no campo de processamento de linguagens naturais, *computer vision*, classificação e processamento de sinais. Sendo mais adotado pela possibilidade de computação gráfica, o domínio de *Deep Learning* ainda possui atributos já vistos nas décadas onde havia poucos avanços no tema. Por exemplo, redes *Transformers* [[vaswani-2017]] utilizam os mesmos mecânimos de atenção e orientação que as redes ART-2 de Grossberg [[ferreira-silva-1998]].

***(!colocar uma imagem minha)***

![[Pasted image 20220802095331.png]] 

É possível verificar que existe uma longa história até chegar ao avanço no desenvolvimento de redes de aprendizado profundo por DP que está no SOTA nos dias atuais. Houve décadas de estagnação e diversas revisões de conceitos, onde algumas ainda estão sendo reutilizadas com a mudança de metodologias. As tarefas de classificação, processamento de sinais e de *computer vision* são diariamente abordadas utilizando métodos já difundidos por essa história evolutiva e novas soluções plausíveis, que, embora sejam totalmente novas, ainda possuem conceitos de décadas atrás, como é possível perceber nas redes *Transformers*.

As próximas seções trarão a conceituação e o funcionamento das redes recorrentes aplicadas ao DL, demonstrando o avanço do tipo de rede neural. Antes que possa demonstrar o funcionamento de tal, é importante realizar o aprofundamento do algoritmo de backpropagation em redes MLP's,  já que é fundamento para a persistência das redes recorrentes.