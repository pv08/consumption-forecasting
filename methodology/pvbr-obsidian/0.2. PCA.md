### PCA
Quando muitos dados estão envolvidos à regressão, uma das boas práticas no pré-processamento é a redução da dimensalidade para aquelas características mais descritivas do *dataset*. Atrelado ao *feature engineering*, que é a operação oposta da redução, a técnica de redução traz aquelas caracteríticas mais correlacionadas, diminuindo a possiblidade da ocorrência da maldição da dimensionalidade, onde o modelo, ao invés de melhorar com o aumento das caracteríristicas envolvidas à variável a ser inferida, piora com o acréscimo de novas [[bellman-1961]].

Muitos trabalho trazem a previsão de série temporais utilizando as técnicas de PCA.  \citeonline{[[elattar-2013]]}, por exemplo, defende a construção de redes neurais compostas pela redução de caracterísitcas, mas encapsuladas em seu kernel. \citeonline{[[zhang-2020]]}, por outro lado, direciona a abordagem ao pré-processamento da base de dados, filtrando aquelas mais importantes à descrição da variável de consumo. Fato esse importante, pois é uma forma construção de *feature selection*, onde o modelo, independe de qual a arquitetura adotada (recorrente ou por *token*), irá receber os dados já tratados vistos mais importantes para a descrição do *dataset* e da variável objeto.

Sabendo que o PCA é efetivamente mais simples de ser usado em etapa de features extraction, é possível verificar a forma como é obtido esses componentes principais. Toda a base de cálculo é derivada do *Single Value Decomposition* (SVD) pela extração de autovetores e autovalores, ou os então melhores referenciados, *eigenvectors* and *eigenvalues*. Assim sendo, dado uma matriz $A_{ij}$, através de sua média será possível obter uma matriz descritiva de sua covariância, demonstrando a pontuação do quão cada característica é predita em relação às demais. Pontuações positivas entre duas variáveis indicam uma dependência entre elas, ou seja, se uma dessas variáveis cresce as outras relacionadas à ela também irão. Pontuações O oposto acontece com pontuações negativas, ou seja, a dependência é o inverso, o acréscimo de uma remete contrário da outra. Já os pontuações nulas, remetem a falta de relação previsível entre elas.

Essa matriz de covariância é ncessária para a construção do *eigenvector*, vetor que ainda descreve a direção descritiva da matriz $A_{ij}$ depois de linearmente transformada,  e *eigenvalue*, medida escalar desse vetor. Dessa forma, sabendo que $Z_{ij}$ é a matriz de pontuações gerada pela covariação da metriz $A_{ij}$ , *eigenvalue* é tido através de ***Equação***, sabendo que $\lambda$ é o *eigenvalue* e $I$ a matriz identidade.

$$ det(Z_{ij}-\lambda I)=0 $$
Através do *eigenvalue* será possível definir os *eigenvectors*, sabendo que é um sistema de equações lineares, o que possiblita a solução pela eliminação de *Gauss*. Sabendo que esses *eigenvectors* são vetores que apontam a direção dos dados em diferentes novas dimensões sem perder sua descritividade,  será necessário escolher a melhor direção. Uma forma de ser realizada essa decisão é realizar o *ranking* desses *eigenvectors* pelos *eigenvalues* e selecionar os $n$ maiores valores que representam melhor o *dataset* nesse novo sub-espaço apontado. Reduzindo então, por exemplo, um espaço tridimensional em bidimensional, será combinado os *eigenvectors* dos dois maiores valores de *eigenvalues*, gerando uma nova matriz $W_{23}$. Essa matriz $W_{ij}$, por fim será combinada à matriz $A_{ij}$ original, exemplificada pela ***Equação***,  para obtenção dos reais valores que compõem os principais componentes.

$$ y=A_{ij}W_{ij}^{-1} $$
