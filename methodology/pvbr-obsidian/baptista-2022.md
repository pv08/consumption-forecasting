@article{baptista-2022,
title = {Relation between prognostics predictor evaluation metrics and local interpretability SHAP values},
journal = {Artificial Intelligence},
volume = {306},
pages = {103667},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103667},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000078},
author = {Marcia L. Baptista and Kai Goebel and Elsa M.P. Henriques},
keywords = {Local interpretability, Model-agnostic interpretability, SHAP values, Monotonicity, Trendability, Prognosability},
abstract = {Maintenance decisions in domains such as aeronautics are becoming increasingly dependent on being able to predict the failure of components and systems. When data-driven techniques are used for this prognostic task, they often face headwinds due to their perceived lack of interpretability. To address this issue, this paper examines how features used in a data-driven prognostic approach correlate with established metrics of monotonicity, trendability, and prognosability. In particular, we use the SHAP model (SHapley Additive exPlanations) from the field of eXplainable Artificial Intelligence (XAI) to analyze the outcome of three increasingly complex algorithms: Linear Regression, Multi-Layer Perceptron, and Echo State Network. Our goal is to test the hypothesis that the prognostics metrics correlate with the SHAP model's explanations, i.e., the SHAP values. We use baseline data from a standard data set that contains several hundred run-to-failure trajectories for jet engines. The results indicate that SHAP values track very closely with these metrics with differences observed between the models that support the assertion that model complexity is a significant factor to consider when explainability is a consideration in prognostics.}
}

[[Bibliography]]