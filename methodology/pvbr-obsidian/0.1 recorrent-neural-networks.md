## Redes Neurais Recorrentes

Com a ideia do funcionamento do algoritmo de treinamento *backpropagation*, exemplificada pela rede *Perceptron Multicamada* (MLP), fica perceptível que a inferência de uma classificação ou regressão se dá pela conexão unitária entre os *perceptrons*, ou seja, existe somente uma ligação entre os neurônios de cada camada. Tal fato remete a percepção de que o neurônio não insere o seu próprio estado anteior como ponderação de um novo peso. Consequentemente, modelos linearmente interligados não possuem consciência de estado adquiridos em treinamentos passados.

O estado do neurônio treinado nessas épocas passadas remete a ideia de memória, uma manifesta importância de inferências passadas na atual, se referindo ao *hidden state* da célula. Tal ponderação ajuda no aprimoramento de reconhecimento dos padrões em problemas, onde a previsão em $y_{t-1}$ é importante para a inferência em $y_{t}$. 

Tarefas de NLP e regressão trazem a percepção de que leituras passadas são impactantes nas presentes. Utilizando a primeira tarefa como exemplo, palavras isoladamente analizadas não possuem qualquer significado. É necessário utilizar um conjunto, guardar o comportamento do neurônio na interpretação dessa palavra isolada e utilizá-lo na próxima amostra a ser verificada. Esse processo é utilizado, por exemplo, na análise de sentimento [[usama-2020]], classificação da qualidade textual em textos em inglês [[liu-2020]] ou a sumarização textual [[chopra-2016]]. A regressão não é diferente, pois leituras passadas são importantes por indicarem sazionalidades e um padrão a ser seguido em um horizonte temporal futuro. Também é perceptível a utilização de RNNs em tarefas de classificação de fraude em cartões de créditos [[mahesh-2022]] ou em consumo de energia [[nadeem-2021]].

Através dessas possiblidades supra-referenciadas, RNNs são amplamente utilizadas quando a inteção é abordar problemas que necessitam de conhecimentos passados para ajudar a inferir o resultado presente. Dessa forma, a próxima seção é direcionada a explicação do avanço do diferentes tipos de arquiteturas voltados a recorrência. Partindo das Vanillas RNN, será possível verificar o problema de Vanishing Gradiant, será possível verificar a solução rede LSTM e GRU e finalizando na transformação de sequências em *tokens* pela rede Transformers, a qual também é amplamente utilizada em tarefas de NLP.

![[Pasted image 20220813132106.png]]


***(!fazer a minha imagem)***
![[Pasted image 20220802102702.png]]

