### LSTM

A proposta de Hochreitter e Schmidhuber [[hochreiter-1997]], em 1997, é uma total nova arquiteutra recorrente. São inseridos o seguintes componentes: $(i)$ uma unidade de memória $c_t$, que servirá como armazenamento de memória, e $(ii)$ um processo decisório que determinará o que será ou não incorporado em tal armazenamento, gerando além da saída, a represetanção desse armazenamento e o estado $h_t$.

O prcesso se divide em três subdecisões:
- 1ª: percepção de que é importante manter a informação gerada anteriormente no armazenamento de memória. Essa ponderação é vista pelo componente chamada *forget gate*, que seguindo o mesmo método de cópia de uma matriz de pesos $W$, incorporará a entrada $x_t$, o hidden-state anteriores $h_{t-1}$ e o bias da referente ao *forget gate* $b_f$ por uma função sigmoidal. O resultado então é multiplicado pelo estado do armazenamento de memória anterior $c_{t-1}$, gerando $\lambda_t$. A equação abaixo demonstra a saída gerada para essa subdecisão: $$f_t=\sigma(x_tW_{xf} + h_{t-1}W_{hf} + b_f)$$ $$ \lambda_t=c_{t-1}\odot f_t $$
- 2ª: percepção de que a nova entrada possui alguma importância para ser guardada no estado de armazenamento $c_t$. Nesta etapa há uma ponderação de um novo candidato utilizando os *input gate* e *candidate memory gate* levando em consideração os mesmos parâmetros utilizados anterioremente, já que eles foram copiados. Assim sendo, será gerado $\alpha_t$, um produto de matriz entre essas duas portas, como demonstrado abaxo $$ 
i_t=\sigma(x_tW_{xt}+h_{t-1}W_{hi}+b_i) $$ $$ \dot{c}=\tanh(x_tW_{xc}+h_{t-1}W_{hc}+b_c) $$ $$ \alpha_t=i_t \odot \dot c $$
Sabendo que a primeira etapa trouxe pelo *forget gate* o que seria ou não descartado do armazenamento anterior, e agora, existe um novo candidato para o armazenamento, é necessário atualizar o armazenamento com o novo candidato. Realizando a soma dessas duas matrizes, será possível obter $c_t$, como visto abaixo $$c_t=\lambda_t + \alpha_t$$
- 3ª: decisão final para o que é importante, por meio do candidato gerado, para gerar o novo estado $h_t$. O novo estado do armazenamento de memória $c_t$ é então repassado a frente e matricialmente multiplicado pelo produto da *output gate*, como descrito abaixo $$ O_t=\sigma(x_tW_{xh} + h_{t-1}W_{ho} + b_o) $$ $$ h_t=tanh(c_t)\odot O_t $$
A saída $y_t$ a ser gerada irá depender do problema a ser abordado. Para o problema de regressão, a saída da rede precisa se passar por uma função de ativação que gerará um único valor e não múltiplos, como na classificação multi-classe. Assim sendo, $y_t$ poderá ser adotado como $y_t=\sigma(W_{hy}h_t)$

O processo de decisão descrito somente defini o processo comparativo entre a saída gerada e o seu respectivo *label* para geração do custo, que para a regressão pode ser o MSE. Tendo o processo de *backpropagation* o mesmo, a rede LSTM evolui as redes recorrentes por inserir o bloco de memória, que poderará o quão importante o novo vetor de entrada é para ser armazenado em uma unidade que já foi filtrada pelo *forget gate*. De tal forma, será obtido atributos de memória a curto e longo prazo, ou seja, atributos mais próximos a entrada quanto da saída, importantes para a devida proporção dos pesos na etapa de *backward*.

![[Pasted image 20220815100853.png]]