### Transformers
Ao realizar a pesquisa sobre a arquitetura Transformers, a maior parte da implementações será direcionada ao domínio de NLP. Porém, ao obter uma pesquisa mais aporfundada sobre sua utilização para a regressão, será possível perceber que sua implementação é bem similar às redes recorrentes,  tendo somente algumas particularidades.

Originalmente proposto pelo time de pesquisa da Google, o modelo Transformers define um *encorder* composto por uma camada de *Self-Attention* ligada à uma arquitetura comum *feedfoward*, os quais possuem o papel de receber múltiplos vetores como entrada, representativos de uma palavra, e transformados em múltiplos vetores representativos do quão fortemente estariam interligados à outros vetores já lidos [[vaswani-2017]], formando então o *Multihead Self-Attention*.  De uma forma geral, o funcionamento dessas camadas são extremamente similares ao das redes recorrentes, porém estariam monopolizadas aos problemas de NLP [[raffel-2020]].

Sabendo disso, o trabalho de [[zerveas-2021]] trata da utilização de Transformers especificamente para problemas de previsão de séries temporais multivariadas, uma das metodologias para constituir o trabalho aqui dissertado. \citeonline([[zerveas-2021]]) explica seu *framework* é uma método genérico para tratativa de dados multivariados, ao contrário de \citeonline([[shiyang-2019]]), um dos trabalhos referenciados por ele que trata previsão de séries temporais, porém univariada.

[[zerveas-2021]] explica o *framework* possui em seu kernel a base do funcionamento do *Transformers*, mas sem o seu respectivo decoder que faria, por meio de *softmax*, a transformação de uma matriz em um vetor correspondente a um número ou palavra. Como a toda a entrada da rede trata-se de um vetor sequencial representativo das características usadas e a quantidade composta por essa sequência. À partir de então, o  funcionamento é o mesmo do Transformers original de \cite([[vaswani-2017]]) , tendo o vetor de entrada transformados em abstrações representativos da entrada, chamados de vetores $q$, $k$ e $v$.

Os vetores representativos $q$ e $v$ serão então multiplicados e divididos por $\sqrt{d_k}$, onde $d_k$ trata-se da dimensionalidade escolhida para representar o vetor $k$, sendo ao final normalizado por uma operação *softmax*. A intenção da ***Equação*** é a construção de um peso para ponderar a sequência de entrada e delinear uma forte ligação com aquelas sequências ligadas à analisada.

$$ w_t = softmax\left(\frac{q\bullet k}{\sqrt{d_k}}\right) $$
Assim como acontece nas redes recorrentes, cada sequência de séries temporais passará por esse processo de *encorder* e será transformado em um *hidden-state* , porém todos sendo representado por um único vetor $\bar{Z}$. A saída $y$, será o mesmo cálculo visto na seçõe [[0.1.1. Vanillas RNN]], uma representação do *hidden-state* por um matriz de peso, como demonstrado na ***Equação***.

$$ y=W_o\bar{Z} $$