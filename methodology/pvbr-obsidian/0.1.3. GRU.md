### GRU

Outra arquitetura que também resolve o problema de *vanishing gradient* das RNN é a GRU. Seu funcionamento é similar à LSTM, mas é uma evolução do que deveria ser a forma como é passado a informação de uma célula para as demais a serem treinadas. 

Tendo a LSTM um estado de memória um estado oculto, que, respectivamente, servem para armazenar informações relevantes e o estado que o neurônio foi treinado, a GRU acopla as duas informações somente no estado oculto da célcula, se tornando mais compacta em relação à LSTM.

São 2 portas de controle, o *reset gate* que fará o papel de filtro, a qual define o quanto do estado $h_{t-1}$ será ou não usado no presente, funcionando de forma análoga ao *input* e *forget gates* da LSTM. Já o *update gate* vai funcionar similarmente ao *output gate* da LSTM pois é onde será decidido o quanto o *hidden state* será atualizado para ser passado adiante.

Tendo início pela etapa realizada pelo *reset gate*, o candidato do que será o novo hidden state $\dot{h}$ será a múltiplicação matricial entre o *hidden state* anterior e o novo vetor de entrada $x_t$, agregados por uma função sigmoidal, como demonstrado abaixo $$ \dot{h}= \tanh(\sigma(x_tW_{th} + h_{t-1}W_{hh} + b_h) \odot h_{t-1}) + (x_tW_{th} + b_x)$$. Após a definição de um novo candidato do *hidden state* $\dot h$, o estado será atualizado, gerando o $Z_t$ em função somente do estado anterior a nova entrada. $$ Z_t=\sigma(x_tW_{th}+h_{t-1}W_{hh}+b{z}) $$Por fim, o *hidden state* gerado será a soma do que foi filtrado e do que será incluso com o novo candidato de *hidden state* $\dot h$. A saída, assim como as demais redes recorrentes até então demonstradas, será uma podenração em relação ao novo *hidden state* gerado. As equações abaixo definam a saída e o novo *hidden state* $$ h_t=(h_{t-1} \odot Z_t) + ([1-Z_t] \odot \dot h) $$
$$ y = W_{yh}h_t $$


![[Pasted image 20220815133639.png]]