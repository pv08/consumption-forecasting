### Vanillas RNN

A premeira etapa da implementação de uma rede recorrente é a chamada Vanilla RNN. A arquitetura segue o princípio de persistência das conexões, havendo estados escondidos $h_t$ ligados a cada cada neurônio. Assim sendo, diferente das redes MLP convencionais, as RNN possuem duas saídas bases, seu resultado e seu estado, representação da memória daquele neurônio. Adotando $h_t$ como o estado para o período, $f$ como função de ativação, $x_t$ vetor de entrada do período e $y_t$ como a saída para o mesmo período, é possível utilizar os mesmos conceitos já vistos no *backpropagation* para o treinamento da rede, como demonstrado na equação abaixo:
$$ h_t=f(h_{t-1};x_t) $$
$h_t$ representa o estado que o neurônio irá adota para o período presenta. Esse estado está em função da entrada corrente e o estado que o neurônio adotou para o período passado. Perceba que é uma formulação incluindo a definição da memória, ou seja, o comportamento do mesmo neurônio para o exemplo anteriormente treinado. Expandindo os parâmetros das funções, é possível verificar na equação abaixo que o estado e a entrada possui uma matrix de pesos de suas respetivas camadas, a escondida$W_{hh}$ e a da entrada, $W_{ih}$ e seus bias. 

$$ h_t=\tanh(h_{t-1}W_{hh} + x_{t}W_{ih} + b_{ih} + b_{hh}) $$
A possibilidade de as redes recorrentes separarem dado não lineares vem da utilização da função de ativação $\tanh$, que é não-linear. Caso houvesse a retirada dessa função de ativação, séria possível verificar que o estado escondido das épocas seria uma representação de camdas lineares, assim como é a MLP, perdendo o atributo de memória das RNN.

A sáida $y_t$ também haverá uma ponderação, que, também é influenciada pela camadas mais internas da redes, porém aqui será uma representação de um novo estado novo que é gerado em função dos passados. A equação abaixo descreve como é descrita essa saída em função do novo estado:

$$ y_t=W_{hy}h_t $$
Como demonstrado na imagem abaixo, é importante detalhar que, há somente uma matriz de pesos $W$. Para cada componente da rede neural recorrente, entrada $x$, estado $h$ e saída $y$, essa mesma matriz será reutilizada. A atuailização dos pesos acontece pela soma desses diferentes atributos. 

(Gerar para o caso de many p um)![[Pasted image 20220815090420.png]]

O treinamento de uma rede recorrente é mais complexo que uma rede MLP por dois fatores: $(i)$ haver o estado do neurônio sendo propagado com os pesos e $(ii)$ as entradas serem representadas como sequências, consequentemente, gerando um número de camadas igual ao número representativo dessa sequência. O segundo fator disperta atenção pela proporção dos ajustes realizados aos pesos, como descrito na Seção de Backpropagation [[0.0.1. backpropagation]]

Após ser comparada à real saída da rede, a rede deve alterar seus pesos das camadas mais internadas movendo-se de forma retrogada até chegar à camda mais próxima da entrada. Alterações muito pequenas nas camadas mais próximas à saída geram alterações  ainda menores nessas camadas mais perto às entradas, acontecendo de forma análoga a alterações mais significativas. 

Como as redes recorrentes utilizam o estado do neurônio anterior para influenciar na previsão do próximo, sequências muito longas geram alterações de pesos muito significativas. Tal fato faz com que o erro da previsão $y-y'$ tenha valores altíssimos que impossibilita a rede aprender, gerando o problema de Vanishing Gradient visto em 1994, por Bengio [[bengio-1994]].

Bengio propôs a alteração da otimização utilizada para o treinamento, como o SA [[bengio-1994]], pois as baseadas em gradientes iram inadequadas para o tipo de rede. Porém, as abordagens direcionadas à alteração da otimização ou reutilização da arquitetura tida até então implementando o estado em uma camada separada, como a de Elman [[elman-1990]], aumentaram a complexidade de tempo e espaço a medida que houve o acréscimo da sequência de entrada. A solução definitiva viria com a proposição da LSTM, alterando os componentes internos do neurônio. Ela será melhor detalhada na seguinte seção [[0.1.2. LSTM]].