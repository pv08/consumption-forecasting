Embora haja a explicação do funcionamento das redes neurais, como visto nas seções \cite{[[0.1 recorrent-neural-networks]]}, \cite{[[0.1.1. Vanillas RNN]]}, \cite{[[0.1.2. LSTM]]}, \cite{[[0.1.3. GRU]]} ou \cite{[[0.1.4. Transformers aplicada à séries temporais]]}, ainda é extremamente custoso saber qual a real influência de cada característica para uma determinada previsão. Tais modelos são muitas vezes referenciados como caixas pretas, os quais recebem um conjunto de entrada para inferir um determinado valor de saída. A avaliação feita entre a relação saída e entrada do sistema, somente diz ao analista o erro e a aderência do modelo aos dados utilizados para o seu treinamento, porém não explicam de forma clara qual o real comportamento das características envolvidas para que a determina inferência seja estabelecida. De forma mais clara, falta uma interpretação microscópica do funcionamento dessa "caixa-preta" e quais são as maiores excitações para uma inferência isolada.

Medidas de erros e a análise de componentes principais por PCA ajudam na parametrização das *features* que mais ajudam na obtenção de um menor erro, mas ainda não é possível através dessas métricas analisar o comportamento isolado dos componentes envolvidos em uma única previsão. É necessário obter técnicas que influenciam  à interpretabilidade do modelo.

Através dessa interpretação é possível, por exemplo, a elaboração de modelos customizadamente construídos para a solução de um dito problema. Para domínios em que não há a criação de uma arquitetura totamente nova, como o tratado no presente trabalho, abodar a interpretabilidade dos modelos, tanto dos já difundidos quanto dos novos, ajudará na obtenção de uma profunda análise de seu comportamento, verificando-se *features* mais importantes para esses condizem com a realidade.

É muito comum haver uma confusão entre os métodos de interpretação e avaliação dos modelos. \citeonline{[[baptista-2022]]} define que interpretar quer dizer mapear localmente o comportamento de cada característica influente no modelo para uma inferência isolada. Diferentemente de avaliar, que também ajuda interpretar a condução das inferências, mas em uma escala macroscópica.

Interpretar o funcionamento dos modelos está no ramo de pesquisa de *eXplainable Artificial Intelligence* (XAI), e são muitos métodos disponíveis para a execução dessa tarefa \cite{[[baptista-2022]]}. LIME \cite{[[ribeiro-2016]]}, um dos métodos mais utilizados, e Captum \cite{[[narine-2020]]}, sendo um dos mais famosos *frameworks* integrativos de algoritmos para o levantamento de métricas desse tipo de análise em *Pytorch*, são algumas das ferramentas para conseguir criar uma metodolgia de interpretação das previsões feitas em domínios de classificação e regressão.

Muitos exemplos de utilização desse método estão disponíveis na literatura. \citeonline{[[zeng-2021]]}, por exemplo, utiliza técnicas de interpretabilidade de modelos de aprendizado profundo para melhorar e explicar a previsão de complicações pós-cirurgicas, naquelas cardiáca congênita pediátricas, por meio de séries temporais de pressão sanguínea e outras características relacionadas a saúde do paciente.

\citeonline{[[baptista-2022]]} e \citeonline{[[zeng-2021]]} utilizam o *SHapely Additive exPlanations Values* (SHAP Values) \cite{[[lundberg-2017]]}, métrica que ajuda na obtenção da importância de cada características para o momento da inferência. Desenvolvida especificamente para os complexos modelos de aprendizado profundo, SHAP Values integra tanto os métodos de \citeonline{[[ribeiro-2016]]} quanto os algoritmos vistos na biblioteca Captum \cite{[[narine-2020]]}, em uma só ferramenta que diponibliza por meio de sua métrica a atribuição do peso de cada *feature* para as inferências isoladas, respeitando a especifidade de cada modelo complexo analisado.
