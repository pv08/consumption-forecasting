### Fusão

A intenção da utilização de modelos de aprendizado profundo é encontrar uma ótima previsão temporal, dependendo do horizonte de previsão utilizado. Assim como outras não referenciadas pelo trabalho, cada uma possui sua metodologia característica, consequentemente, diferentes índices de acuraricidade serão obtidos.

Utilizar métodos de fusão, as quais combinam os diferentes atributos de previsão de cada modelo utilizado para construir essa nova entidade, pode oferece uma solução melhor do que a utilização isolada delas. Sabendo disso, torna-se interessante combinar o melhor de cada modelo com o intuito de diminuir os erros na previsão e realizar um comparativo entre o resultado combinado e aqueles isolados.

Muitos métodos podem ser vistos no SOTA para a combinação desses atributos. Partindo da utilização de média simples, por exemplo, leva em consideração cada modelo de forma equilibrada, o que seria uma desvantagem pelo fato de cada arquitetura possui sua característica dita ineficiente. Por outro lado, métodos de bagging \cite{[[breiman-1996]]}, boosting \cite{[[schapire-1990]]}, stacking \cite{[[breiman-1996-2]]} ou gradient boosting distribuído (XGBootst) \cite{[[wang-2021]]}, mesmo sendo métodos um pouco complexos para a solução procurada, além de haver a necessidade de realizar um novo treinamento para cada *dataset* utilizado, são amplamente utilizados para a tratativa de previsão de séries temporais por encontrar ótimas soluções.

O presente trabalho visa combinar os melhores atributos de cada arquitetura utilizando um método que não tenha um alto custo computacional e que seja relativamente simples para o problema. Verificando essas qualidades, a fusão por média ponderada é vista como uma ótima diretiva, pois não necessita realizar um novo treinamento e cada modelo terá sua devida ponderação de acordo com seu desempenho previamente analisado, sendo necessário uma busca otimizada para a distribuição desses pesos.

Utilizar algoritmo de *grid search*, assim como \citeonline{[[tian-2020]]}, não é interessante pois, dependendo do número de modelos a ser combinado, pode ter uma complexidade inviável para combinar os diferentes pesos para o menor erro. \citeonline{[[taghavi-2015]]} e \citeonline{[[brito-2021]]} tendo essa preocupação de otimização, utilizaram o algoritmo *simulated annealing* para encontrar a melhor ponderação de pesos sem que houvesse viés, desde que a distribuição ocorresse no conjunto de validação \cite{[[brito-2021]]}. Sendo uma ótima solução para a fusão por voto, o algoritmo SA terá seu funcionamento explicado especificamente na próxima seção. 