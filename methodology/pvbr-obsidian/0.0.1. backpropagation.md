### Backpropagation

A formulação de Rosenblatt é extremamente importante para representar o estado de ativação de um neurônio. Utilizando o conjunto fechado de $[0,1]$, a ativação desse neurônio refere-se ao intervalo de 0% à 100% de representatividade. Assim sendo, um neurônio é a representação de um Perceptron.

O modelo *Multi-Layer Perceptron* (MLP) de Hayakin [[hayakin-1994]] determina que para que haja um determinado número de saídas $y_{i}$, é necessário alimentar a rede utilizando um vetor representativo do estado de excitação. Entre a entrada e a saída existe camadas internas de neurônios, importantes na identificação dos padrões e que serão ao longo do treinamento ajustados para definir a real importância para os neurônios das camadas mais externas da rede.

Para um conjunto de exemplo de treinamento, chamado de *mini-batches*, a saída, resultado da ponderação das camadas mais internas, será comparada ao *label*, real resultado que a rede deveria fornecer. Adotando $\alpha_{i}^{L}$ como a saída do neurônio e $y_j$ o resultado esperado, a custo da função $C_{0}$ é dados pela equação: $$ C_{0}=\sum_{i=0}^{n_L-1}(\alpha_{i}^{L}-y_i)^2 $$
A intenção do treinamento da rede é diminuir o custo dessa função, fazendo com que haja também a diminuição do erro entre a variável manipulada, a ativação do neurônio, e o seu *label*. Para que ocorra essaa diminuição, é necessário atualizar os pesos, os bias e o quanto cada neurônio será ativado em todas as camadas internas, preparando cada neurônio do modelo MLP para o recebimento do próximo exemplo a ser verificado. Assim, sabendo que $Z_{i}^{L}$ é a estado do neurônio, ou seja, a relação entre a saída do neurônio da camada anteior e a relação peso e viés do neurônio em análise, como verificado na ***equação abaixo***. Como a saída do neurônio necessariamente previsa de uma função de ativação, podemos adotar sigmoid, como demonstrado na ***segunda equação***
$$ Z_{i}^{L}=w_{ij}\alpha_{i}^{L-1}b_{ij} $$
$$ \alpha_{i}^{L}=\sigma(Z_{i}^{L}) $$

A propagação para as camadas mais internas da rede faz com que haja um efeito em cadeia no ajuste do custo da função, em diferentes proporções. Por exemplo, alterar o valor da saída de um neurônio duas camadas mais internas à saída da rede, pode fazer com que o custo seja alterado em proporções mais elevadas do que foi alterada no neurônio em si. Sabendo disso, é necessário definir a razão entre custo e peso  por meio das relações agragadas à tal, como a saída e o estado do neurônio. Utilizando exemplificamente o que ocorre um neurônio, a razão $\frac{\partial C_{0}}{\partial w_{ik}^{L}}$ pode ser obtida pela derivada das demais, como demonstrado na ***equação abaixo***:
$$ \frac{\partial C_{0}}{\partial w_{ik}^{L}}=\frac{\partial Z_{j}^{L}} {\partial w_{ik}^{L}} \frac{\partial \alpha_{j}^{L}}{\partial Z_{j}^{L}}\frac{\partial C_{0}}{\partial \alpha_{j}^{L}} $$


Performando a derivada de cada componente da relação custo-peso, é possível perceber, por meio da terceira equação, aquela que define a derivada do estado do neurônio em relação ao peso, que a sua saída está diretamente dependente do quão forte é a saída do neurôpnio anteior:

$$\frac{\partial C_0}{\partial \alpha^{L}}=2(\alpha^{L}-y)$$
$$\frac{\partial \alpha^{L}}{\partial w^{L}}=\sigma'(Z^{L})$$
$$\frac{\partial Z^{L}}{\partial w^{L}}=\alpha^{L-1}$$




Podemos perceber que toda a história contada no item anterior é complementar e existem atributos usados até a utilização do modelo *Multi-Layer Percetron*. A formulação matemática de Rosenblatt para a formulação de um *Perceptron*, por exemplo é utilizada, porém interligada em por outras representação de neurônios. O mais interessante é como essa rede irá ajustar-se para que, de acordo com os padrões de sua entrada, possuam a saída fidedigna com a realidade. Através da função representativa do custo da entrada, ou seja, a média das perdas que a rede verificou na relação de entradas e saídas, a própria rede neural, através do algortimo de *backpropagation* irá ajustar seus pesos e bias até que uma melhor saída seja alcançada. 

Tendo um vetor de entrada para a rede neural, cada neurônio, com seu peso e bias iniciais, será ativado de acoordo com o número de neurônios e camadas deles inseridas na rede. Ao final, os valores recém forneceidos por esses neurônios inseridos na camada interna serão parametros de novas entradas para neurônios representativas do número de saídas disponíveis na rede. Os valores, até então escaláveis, serão comparados ao "*label*", representação do gabarito de resposta que, por meio da função de perda, diferença entre a entrada e a saída até então fornecida pelos neurônios pré-iniciados com pesos e bias, será parâmetro para a função de custo definir o quantoos cada peso e bias deve ser alterado. Retroativamente, o erro será fornecido às camadas internas da rede, que ajustará os pesos e viéses dos neurônios das camadas internas e um novo vetor de entrada será disponibilizado. O processo é então repetido até que possa se achar o mínimo global, local onde o peso e o viés é suficientemente adequado para que as saídas sejam o mais próximo do gabarito. A quantidade de neurônios internos e camadas internas é um processo empírico para obtenção do melhor resultado. 

***(! explicar esse passo a passo pela formulação matemárica de https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)***