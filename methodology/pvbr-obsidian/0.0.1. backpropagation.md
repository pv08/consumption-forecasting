### Backpropagation

O modelo *Multi-Layer Perceptron* (MLP) de Hayakin [[hayakin-1994]] determina que, para que haja um determinado número de saídas $y_{i}$, é necessário alimentar a rede utilizando um vetor representativo do estado de excitação. Entre tal entrada e a saída a ser estudada, existem $L$ camadas internas de neurônios, importantes na identificação de padrões e que serão ajustados ao longo do treinamento, ponderando a importância das conexões existentes entre neurônios pelos pesos atribuídos a cada um deles nas camadas mais internas.

Para cada *mini-batches*, pequenos subconjuntos de treinamentos, a saída, resultado da ponderação das camadas mais internas, será comparada ao *label*, real resultado que a rede deveria fornecer. Adotando $\alpha_{i}^{L}$ como a saída do neurônio de uma camada $L$ e $y_j$ o resultado esperado, é possível determinar a função de custo de treinamento $C_{0}$, dada pela equação 1, a qual será diminuída com o treinamento da rede e fará com que o erro entre a variaǘel manipulada e o resultado fornecido pela rede diminua ao decorrer das épocas de treinamento. $$ C_{0}=\sum_{i=0}^{n_L-1}(\alpha_{i}^{L}-y_i)^2 $$
Como todas as camadas internas da rede colaboram para a saída desejada, é necessário atualizar pesos, bias e o nível de ativação de cada neurônio de todas as camadas. Etapa em que uma nova amostra poderá ser considerada como entrada e uma nova saída será gerada pelo modelo MLP. Assim, adotando $Z_{i}^{L}$ como o estado do neurônio, ou seja, a relação entre a saída do neurônio da camada anteior e a relação peso e viés do neurônio em análise, como verificado na ***equação abaixo***., é possível definir $\alpha^{L}_{i}$ por uma função de ativação, como definido por *Rosenblatt*, que neste caso, é uma ativação *sigmoid*, como demonstrado na ***segunda equação***
$$ Z_{i}^{L}=w_{ij}\alpha_{i}^{L-1}b_{ij} $$
$$ \alpha_{i}^{L}=\sigma(Z_{i}^{L}) $$

Treinar uma rede neural significa ajustar os níveis de ativação e estado de cada neurônio, em cada camada, em um processo iterativo. Sendo um ajuste que gera um efeito em cadeia nos neurônios das camadas mais internas, a etapa de propagação de ajuste, *backpropagation*, constitui ajustar pesos, bias e níveis de ativação em diferentes proporções. Por exemplo, alterar o valor da saída de um neurônio duas camadas mais internas à saída da rede, gera uma alteração mais elevada na função do custo. Sabendo disso, é necessário definir a razão entre custo e peso atrelados ao comportamento da MLP. Utilizando, exemplificamente, o que ocorre um neurônio, a razão $\frac{\partial C_{0}}{\partial w_{ik}^{L}}$ pode ser obtida pelo produto das derivadas dos demais componentes, como demonstrado na ***equação abaixo***:
$$ \frac{\partial C_{0}}{\partial w_{ik}^{L}}=\frac{\partial Z_{j}^{L}} {\partial w_{ik}^{L}} \frac{\partial \alpha_{j}^{L}}{\partial Z_{j}^{L}}\frac{\partial C_{0}}{\partial \alpha_{j}^{L}} $$


Por meio da derivada de cada componente relação custo-peso, é possível perceber, por meio da terceira equação, aquela que define a derivada do estado do neurônio em relação ao peso, que a sua saída está diretamente dependente do quão forte é a saída do neurôpnio anteior. Tal percepção remete também à afirmação supracitada de que o treinamento é uma ponderação da importância de neurônio interligados, ou seja, neurônio menos importantes tendem a contribuir menos para saídas mais acuráveis.

$$\frac{\partial C_0}{\partial \alpha^{L}}=2(\alpha^{L}-y)$$
$$\frac{\partial \alpha^{L}}{\partial w^{L}}=\sigma'(Z^{L})$$
$$\frac{\partial Z^{L}}{\partial w^{L}}=\alpha^{L-1}$$

As etapas de leitura da entrada, manifestação de uma saída, comparação com o real, ponderação dos pesos dos neurônios de cada camada e, novamente, a entrada de uma nova amostra é realizada repetidas vezes. O objetivo é realizar o ajuste fino dos pesos diminuindo o custo da função, ou seja, achar a melhor proporção de pesos e bias a fim de obter um menor erroc omparativo entre sáida e *label*. O algoritmo é então avaliado por outro conjunto, chamado então de validação, para que não haja enviesamento das previsões. Somente com bons resultados neste conjunto que é dito que a rede neurael está devidamente treinada para obtenção de resultados nunca vistos.