activation_function: gelu
d_ffn: 256
d_k: 32
d_model: 128
d_v: 32
device: !!python/object/apply:torch.device
- cuda
- 0
fc_dropout: 0.1
lr: 1.0e-05
max_seq_len: 120
n_features: 28
n_head: 16
n_layers: 3
res_dropout: 0.1
scaler: null
seq_len: 60
